---
layout: homework
title: "Assignment #7"
---
<style>
.hl {
	background-color: yellow;
}
img {
    border: 1px solid #000;
}

.warning {
    background-color: yellow;
    color: #aa1122;
    font-weight: bold;
}

.hidden {
    display: none;
}

.hintButton {
    color: #7788ff;
    cursor: pointer;
}
</style>
<script>
document.addEventListener('DOMContentLoaded', hideHints);

function hideHints(evt) {
    document.querySelectorAll('.hint').forEach((ele, i) => {
        const div = document.createElement('div');
        div.id = 'hint' + i + 'Button';
        ele.id = 'hint' + i;
        ele.classList.add('hidden');
        div.addEventListener('click', onClick);
        div.textContent = 'Show Hint';
        div.className = 'hintButton';
        ele.parentNode.insertBefore(div, ele);
    });

}

function onClick(evt) {
    const hintId = this.id.replace('Button', '');
    const hint = document.getElementById(hintId);
    hint.classList.toggle('hidden');
    this.textContent = this.textConent === 'Show Hint' ? 'Hide Hint' : 'Show Hint';
}
</script>

# Assignment #7 - Data from the Web - Due Thursday, Nov 19th at 11pm

In this homework, you'll:

1. Extract Data from Web Pages 
	* Use `urllib` and `BeautifulSoup` to download and parse html
	* Use `pymysql` to insert data into a database
2. Retrieve Data from an API and Optionally Create a Simple Data Visualization

## Part 1 - Load Fall 2020 / Spring 2021 Course into a Database


### Overview

In this part of the assignment, you'll: 

1. Create a data model that holds course data 
2. Programmatically retrieve course data as html pages
3. Extract data from the html retrieved
4. Set up environment on remote server 
5. Configure database access
6. Load the data into a database and run queries

1 - 3 can be done on your computer, but 4 must be done on i6.

### Instructions

#### 1. Create a Data Model


1. Look through the [Spring 2020 CS course schedule](https://cs.nyu.edu/dynamic/courses/schedule/?semester=spring_2020) and [Fall 2020 CS course schedule](https://cs.nyu.edu/dynamic/courses/schedule/?semester=fall_2020) (next semester is not yet up)
2. Create a table or tables to hold the data. You can create a simple single table data model with straightforward columns, or you can create a normalized multi-table model with a non-trivial column mapping. Your discretion! 
	* ⚠️ __The total number of columns in your table (or tables) should exceed the columns in the HTML page table (minimum of 6 columns)__
	* If you're looking for a challenge, try to figure out how to model so that you can query against meeting times (show me all the classes that meet in the morning or all of the classes that only meet once a week)
	* You're free to extract data that isn't apparent based on the HTML table's columns (for example, graduate or undergraduate isn't a column, but it can be inferred)
3. Add your SQL to create a table or tables in `src/create.sql`
	* You do not have to submit an ER diagram
	* ...Though you can use MySQL Workbench to generate generate `CREATE` statements for you

#### 2. Download Data

In  `src/load_courses.py`:

1. Use `urllib` to download the html pages containing course info for the 2020/2021 academic year (fall 2020 and spring 2021)
2. [Check out the slides for a quick guide on using `urllib`](../slides/web/apis.html#4)
3. Find a way to __print out the contents of each course info page__ (this should just be html)


#### 3. Extract Data

In  `src/load_courses.py`:

1. Install `beautifulsoup4` in your environment 
	* `pip3 install beautifulsoup4` or `sudo pip3 install beautifulsoup4` in terminal / `cmd.exe`
	* if you're using Anaconda [follow the docs for installing modules](https://docs.anaconda.com/anaconda/navigator/tutorials/manage-packages/#installing-a-package)
	* if you're using PyCharm [follow the docs for installing modules](https://www.jetbrains.com/help/pycharm/installing-uninstalling-and-upgrading-packages.html)
2. Modify your `load_courses.py` script:
	* parse and extract data from the html pages retrieved by `urllib`
	* use [`beatifulsoup4`](../slides/web/data-formats-web.html) and optionally, the [`re` (regular expressions) module](http://localhost:7000/slides/py/regex.html), to do this
	* the data extracted should be what you'll use to load into your database
	* now, __instead of printing the content of both pages (from the previous part) as the original html__ 
		* print out the extracted data 
		* for example, show the course number, title, instructor and meeting times

#### 4. Set Up a Remote Environment

In this part, you'll have to login to i6

1. Upload your `load_courses.py` file and `create.sql` file to your home directory on `i6` (use scp/winscp from previous assignments to do this, or use an sftp client if you've used one before)... 
	* on MacOS, open terminal and run `scp load_courses.py YOUR_USERNAME@i6.cims.nyu.edu:~`
	* or... on Windows 10, use [winscp](https://winscp.net/eng/index.php) as in previous workshops/assignment
2. Log in to i6 with ssh (as in previous workshops/assignments) 
3. Optionally, create a folder (choose a descriptive name, for example `csci60-hw07`) in your home directory to store your uploaded files and move your files there. On `i6`:
	* create a directory: `mkdir csci60-hw07`
	* move your sql into that directory: `mv create.sql csci60-hw07`
	* move your python script into that directory: `mv load_courses.py csci60-hw07`
	* assuming you're in your home folder, change to that directory: `cd csci60-hw06`
4. Install any modules necessary (because you're on `i6`, you'll have to install modules again, like `beautifulsoup4`:
	* on i6, in any directory: `pip3 install beautifulsoup4`
	* on i6, in any directory: `pip3 install pymysql`
	* (if you're accustomed to having dependencies listed in in a single file, you can save your dependencies in in `requirements.txt` by running `pip3 freeze > requirements.txt` on your development environment and adding that to your repository / upload to i6)

#### 5. Configure Database Access

In this part, you'll have to login to i6

1. Prepare your database and table(s):
	* Use [the database manager](https://cims.nyu.edu/webapps/content/systems/userservices/databases) to create a new database called `courses` (note that this will be prefixed with your username)
	* on `i6`, create your table by running you create statements from `create.sql`
2. On i6, in the same folder that contains `load_courses.py`, add a configuration file
	* when you're on i6, go to the folder that contains your `load_courses.py` file if you're not already in it (use `ls` and `pwd` to see the files in the current directory and the name of the current directory respectively)
	* you'll add a new file, a configuration file; it should contain the information necessary to connect to the database server
	* use a commandline text editor to create your file:
		* `nano NAME_OF_FILE`
		* add text by typing (note that mouse will not work)
		* to save and exit, press the control and x keys together (`ctrl+x`), type Y when prompted to save the file, and finally press enter to accept the file name
	* your configuration can be in `ini` format (like `config.ini` in our lecture demos) or  json (`config.json`):
	* Example format for `config.ini`:
		```
[db]
host = warehouse.cims.nyu.edu
database = USERNAME_DATABASENAME
user = USERNAME
password = PASSWORD
```
	* Example format for `config.json`:
		```
{
	"host":"warehouse.cims.nyu.edu"
	"database":" USERNAME_DATABASENAME"
	"user":"USERNAME"
	"password":"PASSWORD"
}
```
	* Use whichever format you'd like to work with
3. Using `nano` again, modify your `load_courses.py` to load the database configuration from your `ini` or `json`
	* if using an `ini` file:
		```
from  configparser import ConfigParser
```
		```
def get_config(fn):
    parser = ConfigParser()
    parser.read(fn)
    db = parser.items('db')
    return {name: value for name, value in db}
```
		```
conf = get_config('/path/to/config')
print(conf)
```
	* if using `json`, use `open` to read the file and `json.loads(json_string)`... or use `json.load(filename)`

#### 6. Load Data into a Database, Write Queries

On `i6`, using `nano` again, modify your `load_courses.py` to take the data extracted from a previous step and [insert it into the database with `pymysql`](../slides/py-db/mysql.html)

Test your data load:

1. connect to your database with a commandline client
2. write four queries to verify your data load
	1. `DESCRIBE` your table(s)
	2. show the first few rows
	3. write any query #1
	4. write any query #2
3. document your queries and their results:
	* in `src/queries.sql` copy the queries that your wrote
	* under a heading called `Part 1 Queries` (`## Part 1 Queries`)....  copy the output of the queries and paste it into a markdown code block  (surround with three backticks:  
    ```
    ``` add code below these backticks
and above these backticks```
    ```

## Part 2 - Working with an API, Optional Visualization

### Overview

Retrieve data using an API. Optionally use that data in a visualization.

### Instructions

1. Find an API that you're interested in. Here's a  [list of pubic APIs](https://github.com/public-apis/public-apis)
2. In `README.md` document the API that you're using:
	* Under the heading `Part 2 Using an API` (`## Part 2 Using an API`), add:
	* A link to the API's documentation
	* The HTTP Request method and path of each API "endpoint" (url that you'll request) you'll be using
	* The format of the response of the API (is it json?), and sample of it (copy and paste the json that you expect to get back)
	* Specify whether or not you'll need some sort of authentication to retrieve the data
	* If the API offers an official Python client, you can research how to use that instead of using `urllib` or `requests` (for example, [tumblr has `pytumblr`](https://github.com/tumblr/pytumblr)); specify whether or not you'll be doing this
		* If you're using a pre-built client module, link to the module's documentation 
		* Add instructions on how to install the module
	* A short description of what you plan to do with the data... for example, you can:
		* compare the total number of "watchers" between all of the repositories for two github users
		* as we did in class - find the urls of images in the first photo of tumblr photo posts tagged with cat
		* if you'd like, you can [try your hand at creating a visualization with `matlplotlib`](../slides/viz/matplotlib.html)
	* ⚠️ __If you use a tutorial to do this, add the link to the tutorial that you used__
3. In `src/use_api.py` write a script that implements what you described in the `README.md`
	* this script can be run from your own computer (👀 you do not need to be on i6 to run it)
	* if using authentication: 
		* avoid putting your credentials into your source code, and place it in a configuration file instead (using `ConfigParser` with an `ini` file as we did with `pymysql` or use a json file)
		* have git ignore the configuration file by [configuring your repository](https://caltechlibrary.github.io/git-desktop/05-ignore/) 

<br>
<br>
<br>
<br>
