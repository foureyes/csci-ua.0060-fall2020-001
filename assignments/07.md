---
layout: homework
title: "Assignment #7"
---
<style>
.hl {
	background-color: yellow;
}
img {
    border: 1px solid #000;
}

.warning {
    background-color: yellow;
    color: #aa1122;
    font-weight: bold;
}

.hidden {
    display: none;
}

.hintButton {
    color: #7788ff;
    cursor: pointer;
}
</style>
<script>
document.addEventListener('DOMContentLoaded', hideHints);

function hideHints(evt) {
    document.querySelectorAll('.hint').forEach((ele, i) => {
        const div = document.createElement('div');
        div.id = 'hint' + i + 'Button';
        ele.id = 'hint' + i;
        ele.classList.add('hidden');
        div.addEventListener('click', onClick);
        div.textContent = 'Show Hint';
        div.className = 'hintButton';
        ele.parentNode.insertBefore(div, ele);
    });

}

function onClick(evt) {
    const hintId = this.id.replace('Button', '');
    const hint = document.getElementById(hintId);
    hint.classList.toggle('hidden');
    this.textContent = this.textConent === 'Show Hint' ? 'Hide Hint' : 'Show Hint';
}
</script>

# Assignment #7 - Screen Scrape, Database Imports, Using an API / Data Visualization - Due Thursday, Nov 19th at 11pm

In this homework, you'll:

1. "Screen scrape" data from a web site
	* Use `urllib` and `BeautifulSoup` to download and parse html
	* Use `pymysql` to insert data into a database
2. Use an API and Create a Simple Data Visualization

## Part 1 - Load Fall 2020 / Spring 2021 Course into a Database


### Prep

In this part of the assignment, you'll: 


1. Create a data model that holds course data 
2. Programmatically retrieve course data as html pages
3. Extract data from the html retrieved
4. Load the data into a database and run queries

1 - 3 can be done on your computer, but 4 must be done on i6.


### Instructions

#### 1. Create a Data Model


1. Look through the [Fall 2020 CS course schedule](https://cs.nyu.edu/dynamic/courses/schedule/?semester=fall_2020) and [Spring 2021 CS course schedule](https://cs.nyu.edu/dynamic/courses/schedule/?semester=spring_2021) 
2. Create a table or tables to hold the data. You can create a simple single table data model with straightforward columns, or you can create a normalized multi-table model with a non-trivial column mapping. Your discretion! 
	* If you're looking for a challenge, try to figure out how to model so that you can query against meeting times (show me all the classes that meet in the morning or all of the classes that only meet once a week)
	* You're free to extract data that isn't apparent based on the HTML table's columns (for example, graduate or undergraduate isn't a column, but it can be inferred)
3. Add your SQL to create a table or tables in `src/create.sql`
	* You do not have to submit an ER diagram
	* ...Though you can use MySQL Workbench to generate generate `CREATE` statements for you

#### 2. Download Data

In  `src/load_courses.py`:

1. Use `urllib` to download the html pages containing course info for the 2020/2021 academic year (fall 2020 and spring 2021)
2. [Check out the slides for a quick guide on using `urllib`](../slides/web/apis.html#4)
3. Find a way to __print out the contents of each course info page__ (this should just be html)


#### 3. Extract Data

In  `src/load_courses.py`:

1. Install `beautifulsoup4` in your environment 
	* `pip3 install beautifulsoup4` or `sudo pip3 install beautifulsoup4` in terminal / `cmd.exe`
	* if you're using Anaconda [follow the docs for installing modules](https://docs.anaconda.com/anaconda/navigator/tutorials/manage-packages/#installing-a-package)
	* if you're using PyCharm [follow the docs for installing modules](https://www.jetbrains.com/help/pycharm/installing-uninstalling-and-upgrading-packages.html)
2. Modify your `load_courses.py` script:
	* parse and extract data from the html pages retrieved by `urllib`
	* use [`beatifulsoup4`](../slides/web/data-formats-web.html) and optionally, the [`re` (regular expressions) module](http://localhost:7000/slides/py/regex.html), to do this
	* the data extracted should be what you'll use to load into your database
	* now, __instead of printing the content of both pages (from the previous part) as the original html__ 
		* print out the extracted data 
		* for example, show the course number, title, instructor and meeting times

#### 4. Load Data into a Database

In this part, you'll have to login to i6

1. Upload your `load_courses.py` file and `create.sql` file to your home directory on `i6` (use scp/winscp from previous assignments to do this, or use an sftp client if you've used one before)... 
	* on MacOS, open terminal and run `scp load_courses.py YOUR_USERNAME@i6.cims.nyu.edu:~`
	* or... on Windows 10, use [winscp](https://winscp.net/eng/index.php) as in previous workshops/assignment
2. Log in to i6 with ssh (as in previous workshops/assignments) 
3. Optionally, create a folder (choose a descriptive name, for example `csci60-hw07`) in your home directory to store your uploaded files and move your files there. On `i6`:
	* create a directory: `mkdir csci60-hw07`
	* move your sql into that directory: `mv create.sql csci60-hw07`
	* move your python script into that directory: `mv load_courses.py csci60-hw07`
	* assuming you're in your home folder, change to that directory: `cd csci60-hw06`
4. Install any modules necessary (because you're on `i6`, you'll have to install modules again, like `beautifulsoup4`:
	* on i6, in any directory: `pip3 install beautifulsoup4`
	* on i6, in any directory: `pip3 install pymysql`
	* (if you're accustomed to having dependencies listed in in a single file, you can save your dependencies in in `requirements.txt` by running `pip3 freeze > requirements.txt` on your development environment and adding that to your repository / upload to i6)
5. Prepare your database and table(s):
	* Use [the database manager](https://cims.nyu.edu/webapps/content/systems/userservices/databases) to create a new database called `courses` (note that this will be prefixed with your username)
	* on `i6`, create your table by running you create statements from `create.sql`
6. On i6, in the same folder that contains `load_courses.py`, add a configuration file
	* when you're on i6, go to the folder that contains your `load_courses.py` file if you're not already in it (use `ls` and `pwd` to see the files in the current directory and the name of the current directory respectively)
	* you'll add a new file, a configuration file; it should contain the information necessary to connect to the database server
	* use a commandline text editor to create your file:
		* `nano NAME_OF_FILE`
		* add text by typing (note that mouse will not work)
		* to save and exit, press the control and x keys together (`ctrl+x`), type Y when prompted to save the file, and finally press enter to accept the file name
	* your configuration can be in `ini` format (like `config.ini` in our lecture demos) or  json (`config.json`):
	* Example format for `config.ini`:
		```
[db]
host = warehouse.cims.nyu.edu
database = USERNAME_DATABASENAME
user = USERNAME
password = PASSWORD
```
	* Example format for `config.json`:
		```
{
	"host":"warehouse.cims.nyu.edu"
	"database":" USERNAME_DATABASENAME"
	"user":"USERNAME"
	"password":"PASSWORD"
}
```
	* Use whichever format you'd like to work with

	https://cs.nyu.edu/courses/fall20/CSCI-UA.0060-001/_site/slides/web/flask.html#23
7. Using `nano` again, modify your `load_courses.py` to load the database configuration from your `ini` or `json`
	* if using an `ini` file:
		```
from  configparser import ConfigParser
def get_config(fn):
    parser = ConfigParser()
    parser.read(fn)
    db = parser.items('db')
    return {name: value for name, value in db}

conf = get_config('/path/to/config')
print(conf)
```
	* if using `json`, use `open` to read the file and `json.loads(json_string)`... or use `json.load(filename)`
6. Using `nano` again, modify your `load_courses.py` to take the data extracted from a previous step and [insert it into the database with `pymysql`](../slides/py-db/mysql.html)
